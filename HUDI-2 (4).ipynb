{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "There is no current session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 1 define Settings",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%connections hudi-connection\n%glue_version 3.0\n%region us-west-2\n%worker_type G.1X\n%number_of_workers 5\n%extra_jars s3://soumilshah-hudi-demos/jar/deequ-2.0.0-spark-3.1.jar\n%additional_python_modules Faker,pydeequ,sagemaker-pyspark\n%spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "Connections to be included:\nhudi-connection\nSetting Glue version to: 3.0\nPrevious region: us-west-2\nSetting new region to: us-west-2\nReauthenticating Glue client with new region: us-west-2\nIAM role has been set to arn:aws:iam::043916019468:role/Lab3. Reauthenticating.\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::043916019468:role/Lab3\nAuthentication done.\nRegion is set to: us-west-2\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 2\nSetting new number of workers to: 5\nExtra jars to be included:\ns3://soumilshah-hudi-demos/jar/deequ-2.0.0-spark-3.1.jar\nAdditional python modules to be included:\nFaker\npydeequ\nsagemaker-pyspark\nPrevious Spark configuration: spark.serializer=org.apache.spark.serializer.KryoSerializer\nSetting new Spark configuration to: spark.serializer=org.apache.spark.serializer.KryoSerializer\ns3://soumilshah-hudi-demos/jar/deequ-2.0.0-spark-3.1.jar\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 2: Define Imports",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "try:\n    import sys\n    from awsglue.transforms import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.context import SparkContext\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    from pyspark.sql.session import SparkSession\n    from awsglue.dynamicframe import DynamicFrame\n    from pyspark.sql.functions import col, to_timestamp, monotonically_increasing_id, to_date, when\n    from pyspark.sql.functions import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.sql.types import *\n    from datetime import datetime\n    import boto3\n    from functools import reduce\nexcept Exception as e:\n    pass",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 3: Create Spark Session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession, Row, DataFrame\nimport json\nimport pandas as pd\nimport sagemaker_pyspark\n\nimport pydeequ\n\nclasspath = \":\".join(sagemaker_pyspark.classpath_jars())\n\nspark = (SparkSession\n    .builder\n    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \n    .config('spark.sql.extensions', 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension') \n    .config('className', 'org.apache.hudi') \n    .config(\"spark.driver.extraClassPath\", classpath)\n    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n    .getOrCreate())\n\nsc = spark.sparkContext\nglueContext = GlueContext(sc)\njob = Job(glueContext)\nlogger = glueContext.get_logger()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 4: Create Hudi Datalake",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as f\nspark_df = spark.read.parquet(\"s3a://amazon-reviews-pds/parquet/product_category=Electronics/\")\nspark_df = spark_df.withColumn(\"uuid\", f.expr(\"uuid()\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.show(3)\nprint(\"\\n\")\nprint(spark_df.printSchema())",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+----+--------------------+\n|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|year|                uuid|\n+-----------+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+----+--------------------+\n|         US|   52826068|R3SC2T77Y0PSN8|B00FIYDC1W|     822091995|Monster DNA Over-...|          4|            0|          0|   Y|                N|Very nice sound, ...|These headphones ...| 2014-04-09|2014|3ea671d0-e7f4-4ba...|\n|         US|   13676500|R2ONIZ7ICKORQV|B00E19H9U0|     662432872|Tech Armor 30ML G...|          5|            0|          0|   N|                Y|          Five Stars|Arrived as expect...| 2014-11-17|2014|eac611cb-7ac4-4c1...|\n|         US|    9133162|R3RN22LN1VUJR6|B00ELCP55I|     576075415|Mpow&reg; FreeGo ...|          4|            3|          4|   N|                Y|really enjoyed th...|the reason I chos...| 2014-04-09|2014|0247d1d3-9c7a-403...|\n+-----------+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+----+--------------------+\nonly showing top 3 rows\n\n\n\nroot\n |-- marketplace: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- product_parent: string (nullable = true)\n |-- product_title: string (nullable = true)\n |-- star_rating: integer (nullable = true)\n |-- helpful_votes: integer (nullable = true)\n |-- total_votes: integer (nullable = true)\n |-- vine: string (nullable = true)\n |-- verified_purchase: string (nullable = true)\n |-- review_headline: string (nullable = true)\n |-- review_body: string (nullable = true)\n |-- review_date: date (nullable = true)\n |-- year: integer (nullable = true)\n |-- uuid: string (nullable = false)\n\nNone\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Creating Hudi Tables ",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "db_name = \"hudidb\"\ntable_name=\"reviews\"\n\nrecordkey = 'uuid'\nprecombine = 'customer_id'\n\nmethod = 'upsert'\ntable_type = \"COPY_ON_WRITE\"\n\npath = \"s3://soumilshah-hudi-demos/reviews/\"\n\n\nconnection_options={\n    \"path\": path,\n    \"connectionName\": \"hudi-connection\",\n\n    \"hoodie.datasource.write.storage.type\": table_type,\n    'className': 'org.apache.hudi',\n    'hoodie.table.name': table_name,\n    'hoodie.datasource.write.recordkey.field': recordkey,\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': method,\n    'hoodie.datasource.write.precombine.field': precombine,\n\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': db_name,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "WriteDF = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(spark_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"glue_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Data Quailty & Validation  with  PyDeequ for Datalakes ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "ReadGlueDF = (\n    glueContext.create_dynamic_frame.from_options(\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"ReadGlueDF\",\n    )\n)\ndf = ReadGlueDF.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pydeequ.analyzers import *\n\nanalysisResult = AnalysisRunner(spark) \\\n                    .onData(df) \\\n                    .addAnalyzer(Size()) \\\n                    .addAnalyzer(Completeness(\"review_id\")) \\\n                    .addAnalyzer(ApproxCountDistinct(\"review_id\")) \\\n                    .addAnalyzer(Mean(\"star_rating\")) \\\n                    .addAnalyzer(Compliance(\"top star_rating\", \"star_rating >= 4.0\")) \\\n                    .addAnalyzer(Correlation(\"total_votes\", \"star_rating\")) \\\n                    .addAnalyzer(Correlation(\"total_votes\", \"helpful_votes\")) \\\n                    .run()\n                    \nanalysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\nanalysisResult_df.show(truncate=False)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+-------------------------+-------------------+---------------------+\n|entity     |instance                 |name               |value                |\n+-----------+-------------------------+-------------------+---------------------+\n|Column     |review_id                |Completeness       |1.0                  |\n|Column     |review_id                |ApproxCountDistinct|3010972.0            |\n|Mutlicolumn|total_votes,star_rating  |Correlation        |-0.034510979965387205|\n|Dataset    |*                        |Size               |3120938.0            |\n|Column     |star_rating              |Mean               |4.036143941340712    |\n|Column     |top star_rating          |Compliance         |0.7494070692849394   |\n|Mutlicolumn|total_votes,helpful_votes|Correlation        |0.9936463809903744   |\n+-----------+-------------------------+-------------------+---------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "From this, we learn that:\n\n* review_id has no missing values and approximately 3,010,972 unique values.\n* 74.9% of reviews have a star_rating of 4 or higher\n* total_votes and star_rating are not correlated.\n* helpful_votes and total_votes are strongly correlated\n* the average star_rating is 4.0\n* The dataset contains 3,120,938 reviews.\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pydeequ.checks import *\nfrom pydeequ.verification import *\n\ncheck = Check(spark, CheckLevel.Warning, \"Amazon Electronic Products Reviews\")\n\ncheckResult = VerificationSuite(spark) \\\n    .onData(df) \\\n    .addCheck(\n        check.hasSize(lambda x: x >= 3000000) \\\n        .hasMin(\"star_rating\", lambda x: x == 1.0) \\\n        .hasMax(\"star_rating\", lambda x: x == 5.0)  \\\n        .isComplete(\"review_id\")  \\\n        .isUnique(\"review_id\")  \\\n        .isComplete(\"marketplace\")  \\\n        .isContainedIn(\"marketplace\", [\"US\", \"UK\", \"DE\", \"JP\", \"FR\"]) \\\n        .isNonNegative(\"year\")) \\\n    .run()\n\nprint(f\"Verification Run Status: {checkResult.status}\")\ncheckResult_df_all = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\ncheckResult_df_all.show()\ncheckResult_df_all.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "Verification Run Status: Warning\n+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n|Amazon Electronic...|    Warning|     Warning|SizeConstraint(Si...|          Success|                    |\n|Amazon Electronic...|    Warning|     Warning|MinimumConstraint...|          Success|                    |\n|Amazon Electronic...|    Warning|     Warning|MaximumConstraint...|          Success|                    |\n|Amazon Electronic...|    Warning|     Warning|CompletenessConst...|          Success|                    |\n|Amazon Electronic...|    Warning|     Warning|UniquenessConstra...|          Failure|Value: 0.99265669...|\n|Amazon Electronic...|    Warning|     Warning|CompletenessConst...|          Success|                    |\n|Amazon Electronic...|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n|Amazon Electronic...|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n\nroot\n |-- check: string (nullable = true)\n |-- check_level: string (nullable = true)\n |-- check_status: string (nullable = true)\n |-- constraint: string (nullable = true)\n |-- constraint_status: string (nullable = true)\n |-- constraint_message: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "After calling run(), PyDeequ translates your test description into Deequ, which in its turn translates it into a series of Spark jobs which are executed to compute metrics on the data. Afterwards, it invokes your assertion functions (e.g., lambda x: x == 1.0 for the minimum star-rating check) on these metrics to see if the constraints hold on the data.\n\nInterestingly, the review_id column is not unique, which resulted in a failure of the check on uniqueness. We can also look at all the metrics that Deequ computed for this check by running:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "checkResult_df = VerificationResult.successMetricsAsDataFrame(spark, checkResult)\ncheckResult_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+--------------------+------------+------------------+\n| entity|            instance|        name|             value|\n+-------+--------------------+------------+------------------+\n| Column|           review_id|Completeness|               1.0|\n| Column|           review_id|  Uniqueness|0.9926566948782706|\n|Dataset|                   *|        Size|         3120938.0|\n| Column|         star_rating|     Minimum|               1.0|\n| Column|         star_rating|     Maximum|               5.0|\n| Column|marketplace conta...|  Compliance|               1.0|\n| Column|         marketplace|Completeness|               1.0|\n| Column|year is non-negative|  Compliance|               1.0|\n+-------+--------------------+------------+------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "checkResult_df_all.filter(checkResult_df_all.constraint_status == 'Failure').show(truncate=False)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------------------------+-----------+------------+------------------------------------------------------+-----------------+-------------------------------------------------------------------+\n|check                             |check_level|check_status|constraint                                            |constraint_status|constraint_message                                                 |\n+----------------------------------+-----------+------------+------------------------------------------------------+-----------------+-------------------------------------------------------------------+\n|Amazon Electronic Products Reviews|Warning    |Warning     |UniquenessConstraint(Uniqueness(List(review_id),None))|Failure          |Value: 0.9926566948782706 does not meet the constraint requirement!|\n+----------------------------------+-----------+------------+------------------------------------------------------+-----------------+-------------------------------------------------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Credits \n\n* Github Profile  gucciwang for priving a Notebook due to which it was easy to learn concepts \n",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "# References\n\n\nhttps://aws.amazon.com/glue/features/data-quality/\n\n\n\nGetting started with AWS Glue Data Quality from the AWS Glue Data Catalog\nhttps://aws.amazon.com/blogs/big-data/getting-started-with-aws-glue-data-quality-from-the-aws-glue-data-catalog/\n\n\n\nGetting started with AWS Glue Data Quality for ETL Pipelines\nhttps://aws.amazon.com/blogs/big-data/getting-started-with-aws-glue-data-quality-for-etl-pipelines/\n\n\n\nMonitor data quality in your data lake using PyDeequ and AWS Glue\nhttps://aws.amazon.com/blogs/big-data/monitor-data-quality-in-your-data-lake-using-pydeequ-and-aws-glue/\n\n\n\nTesting data quality at scale with PyDeequ\nhttps://aws.amazon.com/blogs/big-data/testing-data-quality-at-scale-with-pydeequ/\n\n\n\n\nTest data quality at scale with PyDeequ\nhttps://github.com/awslabs/python-deequ/blob/master/tutorials/test_data_quality_at_scale.ipynb\n\n\n\n\n",
			"metadata": {}
		}
	]
}